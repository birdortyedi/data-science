{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph_list = []\n",
    "\n",
    "with open('SU-Movie-Reviews-Morp.txt', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        idx = line.split(\" \")[0]\n",
    "        normal = line.split(\" \")[1]\n",
    "        morphed = line.split(\" \")[2]\n",
    "        \n",
    "        morph_list.append((idx, normal, morphed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "with open('SU-Movie-Reviews-Sentences.txt', \"r\", encoding='utf-8') as file:\n",
    "    line = file.read()    \n",
    "    splitted_line = line.split(\"\\n\")\n",
    "    \n",
    "    for s_line in splitted_line[:-1]:\n",
    "        idx = s_line.split(\" \")[0].split(\":\")[-1]\n",
    "        polarity = s_line.split(\" \")[1].split(\":\")[-1]\n",
    "        sentence = \" \".join(s_line.split(\" \")[2:])\n",
    "        \n",
    "        splitted_sentence = re.findall(r\"\\w+|[^\\w\\s]\", sentence, re.UNICODE)\n",
    "        for i in range(len(splitted_sentence)):\n",
    "            for morph in morph_list:\n",
    "                if morph[0] == idx and morph[1] == splitted_sentence[i]:\n",
    "                    splitted_sentence[i] = morph[2]\n",
    "                    break\n",
    "        \n",
    "        morphed_sentence = \" \".join(splitted_sentence)\n",
    "        review_list.append((idx, polarity, sentence, morphed_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SU-Movie-Reviews-KFolds.txt', \"r\", encoding='utf-8') as file:\n",
    "    lines = [line[:-1].strip() for line in file.readlines() if line[:-1] != '']\n",
    "    fold_1_train = [review_list[eval(idx)] for idx in lines[:110][2:]]\n",
    "    fold_1_test = [review_list[eval(idx)] for idx in lines[110:139][1:]]\n",
    "    fold_2_train = [review_list[eval(idx)] for idx in lines[139:249][2:]]\n",
    "    fold_2_test = [review_list[eval(idx)] for idx in lines[249:278][1:]]\n",
    "    fold_3_train = [review_list[eval(idx)] for idx in lines[278:388][2:]]\n",
    "    fold_3_test = [review_list[eval(idx)] for idx in lines[388:417][1:]]\n",
    "    fold_4_train = [review_list[eval(idx)] for idx in lines[417:527][2:]]\n",
    "    fold_4_test = [review_list[eval(idx)] for idx in lines[527:556][1:]]\n",
    "    fold_5_train = [review_list[eval(idx)] for idx in lines[556:666][2:]]\n",
    "    fold_5_test = [review_list[eval(idx)] for idx in lines[666:][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split_data(train_fold, test_fold, morp):\n",
    "    if not morp:\n",
    "        X_train = [review[2] for review in eval(train_fold)]\n",
    "        X_test = [review[2] for review in eval(test_fold)]\n",
    "    else:\n",
    "        X_train = [\" \".join([sp_review_element.split(\"+\")[0] for sp_review_element in review[3].split(\"\\n \")]) \\\n",
    "                  for review in eval(train_fold)]\n",
    "        X_test = [\" \".join([sp_review_element.split(\"+\")[0] for sp_review_element in review[3].split(\"\\n \")]) \\\n",
    "                  for review in eval(test_fold)]\n",
    "    y_train = [review[1] for review in eval(train_fold)]\n",
    "    y_test = [review[1] for review in eval(test_fold)]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(clf, k, morp = False):\n",
    "    scores = []\n",
    "    for i in range(0, k):\n",
    "        train_fold = \"fold_\" + str(i+1) + \"_train\"\n",
    "        test_fold = \"fold_\" + str(i+1) + \"_test\"\n",
    "\n",
    "        X_train, y_train, X_test, y_test = get_split_data(train_fold, test_fold, morp)\n",
    "\n",
    "        clf_ = clf()\n",
    "        vec = TfidfVectorizer(analyzer='word', min_df=0.1, max_df=1.0, sublinear_tf=True, use_idf=True, ngram_range=(1, 2))\n",
    "        vec_clf = Pipeline([('vectorizer', vec), ('clf', clf_)])\n",
    "        vec_clf.fit(X_train, y_train)\n",
    "        scores.append(vec_clf.score(X_test, y_test))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross Validation Scores for normal sentences: \n",
      "[0.35714285714285715, 0.35714285714285715, 0.2857142857142857, 0.42857142857142855, 0.4642857142857143]\n",
      "5-Fold Cross Validation Scores for morphed roots in sentences: \n",
      "[0.5, 0.32142857142857145, 0.42857142857142855, 0.35714285714285715, 0.4642857142857143]\n"
     ]
    }
   ],
   "source": [
    "scores = get_scores(BernoulliNB, 5)\n",
    "print(\"5-Fold Cross Validation Scores for normal sentences: \\n\"+ str(scores))\n",
    "scores_morp = get_scores(BernoulliNB, 5, morp=True)\n",
    "print(\"5-Fold Cross Validation Scores for morphed roots in sentences: \\n\"+ str(scores_morp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for raw data: 0.379 (+/- 0.125)\n",
      "Accuracy for morphological analyzed data: 0.414 (+/- 0.132)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for raw data: %0.3f (+/- %0.3f)\" % (np.mean(scores), np.std(scores) * 2))\n",
    "print(\"Accuracy for morphological analyzed data: %0.3f (+/- %0.3f)\" % (np.mean(scores_morp), np.std(scores_morp) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STN = pd.read_excel(\"./Polarity Resources/SentiTurkNet/STN.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synonyms</th>\n",
       "      <th>Turkish Gloss</th>\n",
       "      <th>Polarity Label</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>neg value</th>\n",
       "      <th>obj value</th>\n",
       "      <th>pos value</th>\n",
       "      <th>Eng Synonyms</th>\n",
       "      <th>English Gloss</th>\n",
       "      <th>SWNpos</th>\n",
       "      <th>SWNneg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14790</th>\n",
       "      <td>kritik</td>\n",
       "      <td>Tehlikeli, endişe veren</td>\n",
       "      <td>o</td>\n",
       "      <td>a</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.068</td>\n",
       "      <td>critical#6</td>\n",
       "      <td>being in or verging on a state of crisis or em...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14791</th>\n",
       "      <td>sönük , loş</td>\n",
       "      <td>ışıksız</td>\n",
       "      <td>o</td>\n",
       "      <td>a</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.057</td>\n",
       "      <td>subdued#5 dim#1</td>\n",
       "      <td>lacking in light; not bright or harsh; \"a dim ...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14792</th>\n",
       "      <td>fok , fok balığı</td>\n",
       "      <td>bir tür deniz balığı</td>\n",
       "      <td>o</td>\n",
       "      <td>n</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.068</td>\n",
       "      <td>pinniped_mammal#1 pinniped#1 pinnatiped#1</td>\n",
       "      <td>aquatic carnivorous mammal having a streamline...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14793</th>\n",
       "      <td>örtü , battaniye</td>\n",
       "      <td>Örtmek için kullanılan şey</td>\n",
       "      <td>o</td>\n",
       "      <td>n</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.068</td>\n",
       "      <td>bedding#1 bedclothes#1 bed_clothing#1</td>\n",
       "      <td>coverings that are used on a bed</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14794</th>\n",
       "      <td>litre , desimetreküp , desimetre küp , l , l. ...</td>\n",
       "      <td>sıvı maddeleri ölçmek için kullnılan birim</td>\n",
       "      <td>o</td>\n",
       "      <td>n</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.068</td>\n",
       "      <td>metric_capacity_unit#1</td>\n",
       "      <td>a capacity unit defined in metric terms</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                synonyms  \\\n",
       "14790                                             kritik   \n",
       "14791                                        sönük , loş   \n",
       "14792                                   fok , fok balığı   \n",
       "14793                                   örtü , battaniye   \n",
       "14794  litre , desimetreküp , desimetre küp , l , l. ...   \n",
       "\n",
       "                                    Turkish Gloss Polarity Label POS tag  \\\n",
       "14790                     Tehlikeli, endişe veren              o       a   \n",
       "14791                                     ışıksız              o       a   \n",
       "14792                        bir tür deniz balığı              o       n   \n",
       "14793                  Örtmek için kullanılan şey              o       n   \n",
       "14794  sıvı maddeleri ölçmek için kullnılan birim              o       n   \n",
       "\n",
       "       neg value  obj value  pos value  \\\n",
       "14790      0.060      0.872      0.068   \n",
       "14791      0.125      0.818      0.057   \n",
       "14792      0.060      0.872      0.068   \n",
       "14793      0.060      0.872      0.068   \n",
       "14794      0.060      0.872      0.068   \n",
       "\n",
       "                                    Eng Synonyms  \\\n",
       "14790                                 critical#6   \n",
       "14791                            subdued#5 dim#1   \n",
       "14792  pinniped_mammal#1 pinniped#1 pinnatiped#1   \n",
       "14793      bedding#1 bedclothes#1 bed_clothing#1   \n",
       "14794                     metric_capacity_unit#1   \n",
       "\n",
       "                                           English Gloss  SWNpos  SWNneg  \n",
       "14790  being in or verging on a state of crisis or em...   0.000   0.125  \n",
       "14791  lacking in light; not bright or harsh; \"a dim ...   0.125   0.750  \n",
       "14792  aquatic carnivorous mammal having a streamline...   0.000   0.000  \n",
       "14793                   coverings that are used on a bed   0.000   0.000  \n",
       "14794            a capacity unit defined in metric terms   0.000   0.000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STN.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STN_dict = dict()\n",
    "\n",
    "for _, syn in STN.iterrows():\n",
    "    STN_dict[syn[0]] = (syn[6], syn[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores_for_STN = []\n",
    "neg_scores_for_STN = []\n",
    "\n",
    "for review in review_list:\n",
    "    words = review[3].split(\" \")\n",
    "    count = 0\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        root = word.split(\"+\")[0]\n",
    "        if root in STN_dict.keys():\n",
    "            count = count + 1\n",
    "            pos_score = pos_score + STN_dict[root][0]\n",
    "            neg_score = neg_score + STN_dict[root][1]\n",
    "   \n",
    "    if count != 0:\n",
    "        avg_pos_score = pos_score / count\n",
    "        avg_neg_score = neg_score / count\n",
    "    else: \n",
    "        avg_pos_score = 0\n",
    "        avg_neg_score = 0\n",
    "    \n",
    "    pos_scores_for_STN.append(avg_pos_score)\n",
    "    neg_scores_for_STN.append(avg_neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SN = pd.read_csv('./Polarity Resources/TurkishSenticNet2.txt', sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>yore</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>geçmiş zamanlar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14223</th>\n",
       "      <td>cry child</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>bağırma çocuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14224</th>\n",
       "      <td>baby need</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>bebek gereksinim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14225</th>\n",
       "      <td>paper punch</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>kâğıt zımbası</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14226</th>\n",
       "      <td>circular route</td>\n",
       "      <td>0.062</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0      1                 2\n",
       "14222            yore -0.023   geçmiş zamanlar\n",
       "14223       cry child -0.132     bağırma çocuk\n",
       "14224       baby need -0.029  bebek gereksinim\n",
       "14225     paper punch -0.035     kâğıt zımbası\n",
       "14226  circular route  0.062               NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SN.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SN_dict = dict()\n",
    "\n",
    "for _, syn in SN.iterrows():\n",
    "    SN_dict[syn[2]] = syn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores_for_SN = []\n",
    "neg_scores_for_SN = []\n",
    "\n",
    "for review in review_list:\n",
    "    words = review[3].split(\" \")\n",
    "    count = 0\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in SN_dict.keys():\n",
    "            count = count + 1\n",
    "            if SN_dict[word] >= 0:\n",
    "                pos_score = pos_score + syn[1]\n",
    "            else:\n",
    "                neg_score = neg_score + syn[1]\n",
    "                \n",
    "    if count != 0:\n",
    "        avg_pos_score = pos_score / count\n",
    "        avg_neg_score = neg_score / count\n",
    "    else: \n",
    "        avg_pos_score = 0\n",
    "        avg_neg_score = 0\n",
    "    \n",
    "    pos_scores_for_SN.append(avg_pos_score)\n",
    "    neg_scores_for_SN.append(avg_neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PWS_NEG = pd.read_csv('./Polarity Resources/allNeg_clean2.txt', header=None)\n",
    "PWS_POS = pd.read_csv('./Polarity Resources/allPos_clean2.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>zoraki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>zorba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>zorlu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>zorunlu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>züğürt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "1263   zoraki\n",
       "1264    zorba\n",
       "1265    zorlu\n",
       "1266  zorunlu\n",
       "1267   züğürt"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWS_NEG.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>gönlü zengin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>gönül</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>hâkim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>sakin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>som</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "916  gönlü zengin\n",
       "917         gönül\n",
       "918         hâkim\n",
       "919         sakin\n",
       "920           som"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PWS_POS.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_indicative_words = []\n",
    "counts_total_words = []\n",
    "counts_for_capital = []\n",
    "\n",
    "indicative_words_lst = [\"izleyin\", \"izle\", \"iyi seyirler\", \"kaçırma\", \"kaçırmayın\", \"izlenmeli\", \"izlemeli\", \"izlemek gerek\", \"izlemek lazım\", \"izlenebilir\"]\n",
    "\n",
    "for review in review_list:\n",
    "    words = review[2].split(\" \")\n",
    "    indic_count = 0\n",
    "    total_count = 0\n",
    "    cap_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in indicative_words_lst:\n",
    "            indic_count = indic_count + 1\n",
    "            \n",
    "        total_count = total_count + 1\n",
    "        \n",
    "        if len(word) > 0:\n",
    "            if word[0].isupper():\n",
    "                cap_count = cap_count + 1\n",
    "                \n",
    "    counts_indicative_words.append(indic_count)\n",
    "    counts_total_words.append(total_count)\n",
    "    counts_for_capital.append(cap_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PWS_dict = dict()\n",
    "\n",
    "for pws in PWS_POS.iterrows():\n",
    "    PWS_dict[pws[1].item()] = 1\n",
    "    \n",
    "for pws in PWS_NEG.iterrows():\n",
    "    PWS_dict[pws[1].item()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_counts_for_PWS = []\n",
    "neg_counts_for_PWS = []\n",
    "counts_for_adj_and_adv = []\n",
    "is_negated_sentence = []\n",
    "\n",
    "for review in review_list:\n",
    "    words = review[3].split(\" \")\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    adj = 0\n",
    "    adv = 0\n",
    "    is_negated = False\n",
    "    \n",
    "    for word in words:\n",
    "        root = word.split(\"+\")[0]\n",
    "        if root in PWS_dict.keys():\n",
    "            if PWS_dict[root] == 1:\n",
    "                pos_count = pos_count + 1\n",
    "            else:\n",
    "                neg_count = neg_count + 1\n",
    "                \n",
    "        if \"Adj\" in word:\n",
    "            adj = adj + 1\n",
    "        if \"Adverb\" in word:\n",
    "            adv = adv + 1\n",
    "        if \"Neg\" in word:\n",
    "            is_negated = True\n",
    "    \n",
    "    pos_counts_for_PWS.append(pos_count)\n",
    "    neg_counts_for_PWS.append(neg_count)\n",
    "    counts_for_adj_and_adv.append(adj+adv)\n",
    "    is_negated_sentence.append(1 if is_negated else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.vstack((pos_scores_for_STN, neg_scores_for_STN, pos_scores_for_SN, neg_scores_for_SN, pos_counts_for_PWS, neg_counts_for_PWS, counts_for_adj_and_adv, counts_indicative_words, counts_for_capital, counts_total_words, is_negated_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame(normalize(features.T, axis=0, norm='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_df.columns = [\"POS_STN\", \"NEG_STN\", \"POS_SN\", \"NEG_SN\", \"POS_PWS\", \"NEG_PWS\", \"ADJ_ADV\", \"INDIC_WORDS\", \"INIT_CAPITAL\", \"TOTAL_TOKENS\", \"NEGATED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_STN</th>\n",
       "      <th>NEG_STN</th>\n",
       "      <th>POS_SN</th>\n",
       "      <th>NEG_SN</th>\n",
       "      <th>POS_PWS</th>\n",
       "      <th>NEG_PWS</th>\n",
       "      <th>ADJ_ADV</th>\n",
       "      <th>INDIC_WORDS</th>\n",
       "      <th>INIT_CAPITAL</th>\n",
       "      <th>TOTAL_TOKENS</th>\n",
       "      <th>NEGATED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.074791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>0.06800</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>0.37350</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>0.06800</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      POS_STN   NEG_STN  POS_SN  NEG_SN  POS_PWS  NEG_PWS   ADJ_ADV  \\\n",
       "2666  0.05375  0.074791     0.0     0.0      0.0      0.0  0.027027   \n",
       "2667  0.06800  0.062762     0.0     0.0      0.0      0.2  0.054054   \n",
       "2668  0.37350  0.062762     0.0     0.0      0.0      0.0  0.243243   \n",
       "2669  0.06800  0.062762     0.0     0.0      0.0      0.2  0.189189   \n",
       "2670  0.00000  0.000000     0.0     0.0      0.0      0.0  0.000000   \n",
       "\n",
       "      INDIC_WORDS  INIT_CAPITAL  TOTAL_TOKENS  NEGATED  \n",
       "2666          0.0      0.028571      0.091667      1.0  \n",
       "2667          0.0      0.000000      0.083333      0.0  \n",
       "2668          0.0      0.028571      0.283333      1.0  \n",
       "2669          0.0      0.085714      0.258333      0.0  \n",
       "2670          0.0      0.028571      0.025000      0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "for review in review_list:\n",
    "    reviews.append(review[2])\n",
    "    labels.append(review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', min_df=0.1, max_df=1.0, sublinear_tf=True, use_idf=True, ngram_range=(1, 2))\n",
    "reviews = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:2].values), axis=1) # Accuracy: 0.457 (+/- 0.095)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,2:4].values), axis=1) # Accuracy: 0.414 (+/- 0.160)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,4:6].values), axis=1) # Accuracy: 0.414 (+/- 0.184)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:6].values), axis=1) # Accuracy: 0.464 (+/- 0.090)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,6:7].values), axis=1) # Accuracy: 0.407 (+/- 0.140)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:7].values), axis=1) # Accuracy: 0.464 (+/- 0.090)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,7:8].values), axis=1) # Accuracy: 0.414 (+/- 0.160)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:8].values), axis=1) # Accuracy: 0.464 (+/- 0.090)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,8:9].values), axis=1) # Accuracy: 0.414 (+/- 0.160)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:9].values), axis=1) # Accuracy: 0.464 (+/- 0.090)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,9:10].values), axis=1) # Accuracy: 0.407 (+/- 0.107)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,:10].values), axis=1) # Accuracy: 0.464 (+/- 0.090)\n",
    "#data = np.concatenate((reviews.toarray(), f_df.iloc[:,10:11].values), axis=1) # Accuracy: 0.407 (+/- 0.107)\n",
    "data = np.concatenate((reviews.toarray(), f_df.iloc[:,:].values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2671, 19)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('SU-Movie-Reviews-KFolds.txt', \"r\", encoding='utf-8') as file:\n",
    "    lines = [line[:-1].strip() for line in file.readlines() if line[:-1] != '']\n",
    "    fold_1_train_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[:110][2:]]\n",
    "    fold_1_test_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[110:139][1:]]\n",
    "    fold_2_train_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[139:249][2:]]\n",
    "    fold_2_test_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[249:278][1:]]\n",
    "    fold_3_train_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[278:388][2:]]\n",
    "    fold_3_test_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[388:417][1:]]\n",
    "    fold_4_train_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[417:527][2:]]\n",
    "    fold_4_test_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[527:556][1:]]\n",
    "    fold_5_train_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[556:666][2:]]\n",
    "    fold_5_test_lr = [(data[eval(idx)],labels[eval(idx)]) for idx in lines[666:][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_split_data_lr(train_fold, test_fold):\n",
    "    X_train = [review for review, label in eval(train_fold)]\n",
    "    X_test = [review for review, label in eval(test_fold)]\n",
    "    y_train = [label for review, label in eval(train_fold)]\n",
    "    y_test = [label for review, label in eval(test_fold)]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_lr = [] \n",
    "for i in range(0, 5):\n",
    "    train_fold = \"fold_\" + str(i+1) + \"_train_lr\"\n",
    "    test_fold = \"fold_\" + str(i+1) + \"_test_lr\"\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_split_data_lr(train_fold, test_fold)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    scores_lr.append(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.39285714285714285, 0.5, 0.5, 0.5357142857142857]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.486 (+/- 0.097)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(scores_lr), np.std(scores_lr) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For random cross validation folded with all data \n",
    "random_cv_scores = cross_val_score(BernoulliNB(), data, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62242991,  0.54766355,  0.57865169,  0.58801498,  0.57786116])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.583 (+/- 0.048)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(random_cv_scores), np.std(random_cv_scores) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
