{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "filepath = 'C://Users//fk2969//Desktop//twitter_sentiment_corpus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the dataset with Pandas\n",
    "dataset = pd.read_csv(filepath)\n",
    "\n",
    "# Extracting irrelevent sentiments\n",
    "dataset = dataset[dataset.Sentiment != 'irrelevant']\n",
    "\n",
    "# Splitting the data into tweets and sentiments\n",
    "tweets = dataset[\"TweetText\"]\n",
    "sentiments = dataset[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handling URLs on tweets\n",
    "def handle_url(tweet):\n",
    "    tweet = re.sub('(http:\\/\\/.*[\\r\\n]*)', '__URL', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handling usernames on tweets\n",
    "def handle_username(tweet):\n",
    "    tweet = re.sub('@[^\\s]+','__USERNAME',tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handling hashtags on tweets\n",
    "def handle_hashtag(tweet):\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making all words on tweets proper\n",
    "def handle_words(tweet):\n",
    "    tweet = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE).sub(r\"\\1\\1\", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling emoticons on tweets\n",
    "emoticons = \\\n",
    "    [\n",
    "     ('i like it',[ ':-)', ':)', '(:', '(-:', \\\n",
    "                       ':-D', ':D', 'X-D', 'XD', 'xD', \\\n",
    "                       ':-P', ':P', 'X-P', 'XP', 'xP', \\\n",
    "                       '<3', ':\\*', ';-)', ';)', ';-D', ';D', '(;', '(-;', ] ),\\\n",
    "     ('i do not like it', [':-(', ':(', '):', ')-:', ':,(',\\\n",
    "                       ':\\'(', ':\"(', ':((', \n",
    "                        ':@', ':-@'] ),\\\n",
    "    ]\n",
    "    \n",
    "def replace_parenth(arr):\n",
    "        return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "    \n",
    "def regex_join(arr):\n",
    "        return '(' + '|'.join( arr ) + ')'\n",
    "    \n",
    "def handle_emoticons(tweet):\n",
    "    emoticons_regex = [(repl, re.compile(regex_join(replace_parenth(regx))) ) \\\n",
    "            for (repl, regx) in emoticons]\n",
    "    \n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        tweet = re.sub(regx, ' '+repl+' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stemming a tweet using \"nltk\" library\n",
    "def tweet_stemmer(tweet):\n",
    "    tweet_stem = ' '\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    words = [word if(word[0:2]=='__') else word.lower() for word in tweet.split() if len(word) >= 2]\n",
    "    words = [stemmer.stem(word) for word in words] \n",
    "    tweet_stem = ' '.join(words)\n",
    "    return tweet_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "def preprocess_data(tweets, sentiments):\n",
    "    X = []; y= []\n",
    "    for tweet, sentiment in zip(tweets, sentiments):\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "        tweet = handle_url(tweet)\n",
    "        tweet = handle_username(tweet)\n",
    "        tweet = handle_hashtag(tweet)\n",
    "        tweet = handle_words(tweet)\n",
    "        tweet = handle_emoticons(tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet_stem = tweet_stemmer(tweet)\n",
    "\n",
    "        X.append(tweet_stem)\n",
    "        y.append(sentiment)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the Random Forest Classifier model\n",
    "# svm_clf = SVC(C=0.1, kernel='rbf') # Accuracy: 0.456 +/- 0.001\n",
    "# svm_clf = SVC(C=1, kernel='linear') # Accuracy: 0.698 +/- 0.072\n",
    "# nb_clf = BernoulliNB() # Accuracy: 0.624 +/- 0.195\n",
    "# nb_clf = MultinomialNB() # Accuracy: 0.702 +/- 0.053\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, criterion='gini', \\\n",
    "                                max_depth=None, min_samples_split=4, \\\n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, \\\n",
    "                                max_features='auto', max_leaf_nodes=None, \\\n",
    "                                bootstrap=True, oob_score=True, n_jobs=-1, \\\n",
    "                                random_state=43, verbose=1, \\\n",
    "                                warm_start=True, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=...stimators=200, n_jobs=-1, oob_score=True, random_state=43,\n",
       "            verbose=1, warm_start=True))])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vectorizer in pipeline \n",
    "X, y = preprocess_data(tweets, sentiments)\n",
    "vec = TfidfVectorizer(min_df=10, max_df=0.95, sublinear_tf=True, use_idf=True, ngram_range=(1, 2))\n",
    "vec_clf = Pipeline([('vectorizer', vec), ('pac', rf_clf)])\n",
    "vec_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-baggage Score: 0.761\n"
     ]
    }
   ],
   "source": [
    "print(\"Out-of-baggage Score: %0.3f\" % rf_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   10.0s remaining:    6.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold Cross Validation Scores: \n",
      "[ 0.6744186   0.67151163  0.75510204  0.76608187  0.6871345   0.73099415\n",
      "  0.69005848  0.70175439  0.70175439  0.68035191]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Validating the model with K-Fold Cross Validation method\n",
    "scores = cross_val_score(vec_clf, X, y, cv=10, n_jobs=-1, verbose=1)\n",
    "print(\"10-Fold Cross Validation Scores: \\n\"+ str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.706 (+/- 0.064)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
